#!/usr/bin/env python3
"""
GPU/CUDA VerfÃ¼gbarkeits-Check fÃ¼r Planvision
PrÃ¼ft ob CUDA/GPU fÃ¼r PyTorch verfÃ¼gbar ist
"""

import torch
import sys

def check_gpu_support():
    print("ğŸ” GPU/CUDA Support Check fÃ¼r Planvision")
    print("=" * 50)
    
    # PyTorch Version
    print(f"ğŸ PyTorch Version: {torch.__version__}")
    
    # CUDA VerfÃ¼gbarkeit
    cuda_available = torch.cuda.is_available()
    print(f"ğŸš€ CUDA verfÃ¼gbar: {'âœ… JA' if cuda_available else 'âŒ NEIN'}")
    
    if cuda_available:
        # CUDA Details
        cuda_version = torch.version.cuda
        print(f"ğŸ“¦ CUDA Version: {cuda_version}")
        
        # GPU Informationen
        gpu_count = torch.cuda.device_count()
        print(f"ğŸ–¥ï¸  GPU Anzahl: {gpu_count}")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB VRAM)")
        
        # Performance Test
        print("\nâš¡ GPU Performance Test...")
        try:
            # Erstelle Test-Tensor auf GPU
            device = torch.device('cuda:0')
            test_tensor = torch.randn(1000, 1000, device=device)
            result = torch.matmul(test_tensor, test_tensor)
            print("âœ… GPU Test erfolgreich - GPU ist einsatzbereit!")
            
            # Empfohlene Konfiguration
            print(f"\nğŸ¯ Empfohlene Einstellung fÃ¼r model_handler.py:")
            print(f"   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')")
            
            # GeschÃ¤tzte Performance-Verbesserung
            print(f"\nğŸ“ˆ Erwartete Performance-Verbesserung:")
            print(f"   - Model-Inferenz: 50-70% schneller")
            print(f"   - Gesamt-Request: 30-50% schneller")
            
        except Exception as e:
            print(f"âŒ GPU Test fehlgeschlagen: {e}")
            print("   GPU ist verfÃ¼gbar aber nicht funktionsfÃ¤hig")
    else:
        print("\nğŸ’¡ Ohne GPU:")
        print("   - Aktuelle CPU-Performance ist ok fÃ¼r Development")
        print("   - FÃ¼r Production: GPU-Host empfohlen fÃ¼r bessere UX")
        print("   - Alternative: Model-Quantisierung oder kleineres Modell")
    
    # MPS Support (Apple Silicon)
    mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()
    if mps_available:
        print(f"\nğŸ Apple Silicon MPS: âœ… VerfÃ¼gbar")
        print("   - Kann fÃ¼r moderate Beschleunigung genutzt werden")
        print("   - device = torch.device('mps')")
    
    # Empfehlung
    print("\n" + "=" * 50)
    if cuda_available:
        print("ğŸ¯ EMPFEHLUNG: GPU-Beschleunigung aktivieren!")
        print("   â†’ Deutlich schnellere Fenstererkennung")
    elif mps_available:
        print("ğŸ¯ EMPFEHLUNG: MPS-Beschleunigung aktivieren!")
        print("   â†’ Moderate Verbesserung auf Apple Silicon")
    else:
        print("ğŸ¯ EMPFEHLUNG: CPU-Optimierungen fokussieren")
        print("   â†’ Modell-Quantisierung, kleinere BildgrÃ¶ÃŸen, Async Processing")
    
    return cuda_available or mps_available

if __name__ == "__main__":
    try:
        gpu_support = check_gpu_support()
        sys.exit(0 if gpu_support else 1)
    except Exception as e:
        print(f"âŒ Fehler beim GPU-Check: {e}")
        sys.exit(1)